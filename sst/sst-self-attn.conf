[hparam]

############ dataset #############
dataset = SST
lower = False
fine_grained = False


######### word embedding #########
pretrained = glove.840B.300d


############# model ##############
# word embedding
word_vec_size = 300
fix_word_emb = False

# Self attention
encoder = SelfAttn
layer = 1
brnn = True
mem_size = 300
attn_size = 300
merge = all
hops = 1
penalization_coeff = 0

# softmax classifier
hid_sizes_cls = 300,

# parameters initialization
param_init = 0.002


############ training ############
epochs=20
batch_size = 32
lr = 0.0005
optim = rmsprop
learning_rate_decay = 0.95
weight_decay = 0
# start lr decay at this epoch
reduce_lr_at = 5
dropout = 0.5


########### evaluation ############
# how many times of evaluation in one epoch
eval_epoch = 20
# how many times of printing information in one epoch
interval = 3
